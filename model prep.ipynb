{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe85ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import utils\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3d10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b59cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c524edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 224\n",
    "NUM_CLASSES = 185\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "best_prec1 = 0\n",
    "classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68e70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectModel(MODEL_ID):\n",
    "    if MODEL_ID == 1:\n",
    "        BATCH_SIZE = 128\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES) #nn.Linear(input_size, num_classes)\n",
    "        modelName = \"resnet18_augment\"\n",
    "    elif MODEL_ID == 2:\n",
    "        BATCH_SIZE = 128\n",
    "        NUM_EPOCHS = 72\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES) #nn.Linear(input_size, num_classes)\n",
    "        modelName = \"resnet18_decay_adam\"\n",
    "    elif MODEL_ID == 3:\n",
    "        BATCH_SIZE = 128\n",
    "        NUM_EPOCHS = 50\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.VGG('VGG16')\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES)\n",
    "        modelName = \"VGG16\"\n",
    "    elif MODEL_ID == 4:\n",
    "        BATCH_SIZE = 64\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-0 #start from learning rate after 40 epochs\n",
    "        ALPHA = 10\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Linear(2048, NUM_CLASSES)\n",
    "        modelName = \"resnet50\"\n",
    "    elif MODEL_ID == 5:\n",
    "        BATCH_SIZE = 8\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.densenet121()\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES)\n",
    "        modelName = \"densenet121\"\n",
    "    elif MODEL_ID == 6:\n",
    "        BATCH_SIZE = 1024\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = nn.Sequential()\n",
    "        model.add_module(\"linear\", torch.nn.Linear(224*224*3, NUM_CLASSES, bias=False))\n",
    "        # RuntimeError: size mismatch, m1: [172032 x 224], m2: [150528 x 185] at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorMathBlas.cu:249\n",
    "        # model = nn.Linear(224*224*3, NUM_CLASSES) \n",
    "        #error size mismatch, m1: [86016 x 224], m2: [150528 x 185] at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorMathBlas.cu:249\n",
    "        modelName = \"logisticRegression\"\n",
    "    else:\n",
    "        raise ValueError('Model ID must be an integer between 1 and 6')\n",
    "    return model, modelName, BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, ALPHA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ea91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data file with header\n",
    "def createHeadertxt_train(modelName, INPUT_SIZE, filename):\n",
    "    with open(filename, 'a') as a:\n",
    "        a.write('#Epoch\\t\\t   Time\\t\\t            Data\\t\\t   Loss\\t\\t\\t\\t   Prec@1\\t   Prec@5 \\n')\n",
    "\n",
    "def createHeadertxt_dev(modelName, INPUT_SIZE, filename):\n",
    "    with open(filename, 'a') as a:\n",
    "        a.write('#Epoch\\t\\t    Time\\t\\t    Loss\\t\\t   Prec@1\\t\\t   Prec@5 \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c540c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method which trains model for 1 epoch\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # for i, (input, target) in enumerate(train_loader):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        (input,target),(path,_) = data\n",
    "        # measure data loading time\n",
    "        if USE_CUDA:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        if MODEL_ID ==6:\n",
    "            ##flatten input for logistic\n",
    "            input_var = input_var.view(-1,INPUT_SIZE*INPUT_SIZE*3)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5), path=path, minibatch = i)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  '\\Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1, top5=top5)) \n",
    "\n",
    "        if i == (len(train_loader)-1):\n",
    "            with open(filename_train, 'a') as a:\n",
    "                    a.write('{0}\\t'\n",
    "                            '{batch_time.avg:16.3f}\\t'\n",
    "                            '{data_time.avg:16.3f}\\t'\n",
    "                            '{loss.avg:16.4f}\\t'\n",
    "                            '{top1.avg:16.3f}\\t'\n",
    "                            '{top5.avg:16.3f}\\n'.format(\n",
    "                                epoch, batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses, top1=top1, top5=top5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43465b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation method\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "#    class_correct = list(0. for i in range(NUM_CLASSES))\n",
    "#    class_total = list(0. for i in range(NUM_CLASSES))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if USE_CUDA:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "        with torch.no_grad(): \n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        if MODEL_ID ==6:\n",
    "            ##flatten input for logistic\n",
    "            input_var = input_var.view(-1,INPUT_SIZE*INPUT_SIZE*3)\n",
    "\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1, top5=top5))\n",
    "        if i == (len(val_loader)-1):    \n",
    "            with open(filename_dev, 'a') as a:\n",
    "                    a.write('{0}\\t'\n",
    "                            '{batch_time.avg:16.3f}\\t'\n",
    "                            '{loss.avg:16.4f}\\t'\n",
    "                            '{top1.avg:16.3f}\\t'\n",
    "                            '{top5.avg:16.3f}\\n'.format(\n",
    "                                epoch, batch_time=batch_time,\n",
    "                                loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cedccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print('\\n[INFO] Saved Model to model_best.pth.tar')\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55523da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LEARNING_RATE*0.1** (epoch // ALPHA)\n",
    "    if (lr <= 1e-4): # cap the learning rate to be larger than e-4\n",
    "        lr = 1e-4\n",
    "    print('\\n[Learning Rate] {:0.6f}'.format(lr))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6694023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,), path = None, minibatch = None):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    ## save mislabeled data\n",
    "    if path:\n",
    "        filename = [os.path.basename(p) for p in path]\n",
    "        true_label = [os.path.basename(os.path.dirname(p)) for p in path]\n",
    "        ##debugging for densenet\n",
    "        # print('pred[0] = %s'%pred[0])\n",
    "        ## end debugging\t\t\n",
    "        pred_label = [classes[p] for p in pred[0]]\n",
    "        data = np.array([filename, true_label, pred_label])\n",
    "        out = pd.DataFrame(data.T,columns =['filename', 'true_label','pred_label'])\n",
    "        out.index.name = 'index'\n",
    "        out['correct?'] = out['pred_label']==out['true_label']\n",
    "        out_file = 'predicted_labels.csv'\n",
    "\n",
    "        if os.path.isfile(out_file):\n",
    "            if minibatch==0: # if first minibatch, overwrite existing file\n",
    "                out.to_csv(out_file)\n",
    "            else:\n",
    "                df = pd.read_csv(out_file, index_col = 0)\n",
    "                df = pd.concat([df,out],axis = 0, ignore_index = True)\n",
    "                df.to_csv(out_file)\n",
    "        else: # if file does not exist, make file\n",
    "            out.to_csv(out_file)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f4cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImageFolder(datasets.ImageFolder): #return image path and loader\n",
    "    def __getitem__(self, index):\n",
    "        return super(MyImageFolder, self).__getitem__(index), self.imgs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca5cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206b6ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Creating Model\n",
      "\n",
      "[INFO] Reading Training and Testing Dataset\n",
      "\n",
      "[INFO] Preparing txt files to save epoch data\n",
      "\n",
      "[INFO] Training Started\n",
      "\n",
      "[Learning Rate] 1.000000\n"
     ]
    }
   ],
   "source": [
    "print('\\n[INFO] Creating Model')\n",
    "model, modelName, BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, ALPHA = selectModel(MODEL_ID)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if USE_CUDA:\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    criterion = criterion.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE,\n",
    "                     momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "#                       eps=1e-08, weight_decay=1e-4)\n",
    "\n",
    "print('\\n[INFO] Reading Training and Testing Dataset')\n",
    "traindir = os.path.join(\"dataset_all/leafsnap/dataset/train\")\n",
    "testdir = os.path.join(\"dataset_all/leafsnap/dataset/test\")\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "data_train = MyImageFolder(traindir, transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]))\n",
    "data_test = datasets.ImageFolder(testdir, transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize]))\n",
    "classes = data_train.classes\n",
    "classes_test = data_test.classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(data_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=4) \n",
    "\n",
    "print('\\n[INFO] Preparing txt files to save epoch data')\n",
    "timestamp_string = time.strftime(\"%Y%m%d-%H%M%S\") \n",
    "filename_train = './dataAndPlots/' + timestamp_string + '_train' + '_' + modelName + '_' + str(INPUT_SIZE) + '.txt'\n",
    "filename_dev = './dataAndPlots/' + timestamp_string + '_dev' + '_' + modelName + '_' + str(INPUT_SIZE) + '.txt'\n",
    "createHeadertxt_train(modelName, INPUT_SIZE, filename_train)\n",
    "createHeadertxt_dev(modelName, INPUT_SIZE, filename_dev)\n",
    "\n",
    "print('\\n[INFO] Training Started')\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    print('\\n[INFO] Saved Model to leafsnap_model.pth')    \n",
    "    torch.save(model, 'leafsnap_model.pth')\n",
    "\n",
    "print('\\n[DONE]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2f5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
