{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c92b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "from averagemeter import AverageMeter\n",
    "from models import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119821a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL CONSTANTS\n",
    "INPUT_SIZE = 224\n",
    "NUM_CLASSES = 185\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "best_prec1 = 0\n",
    "classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6403d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc36327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectModel(MODEL_ID):\n",
    "    if MODEL_ID == 1:\n",
    "        BATCH_SIZE = 128\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES) #nn.Linear(input_size, num_classes)\n",
    "        modelName = \"resnet18_augment\"\n",
    "    elif MODEL_ID == 2:\n",
    "        BATCH_SIZE = 128\n",
    "        NUM_EPOCHS = 72\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES) #nn.Linear(input_size, num_classes)\n",
    "        modelName = \"resnet18_decay_adam\"\n",
    "    elif MODEL_ID == 3:\n",
    "        BATCH_SIZE = 128\n",
    "        NUM_EPOCHS = 50\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.VGG('VGG16')\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES)\n",
    "        modelName = \"VGG16\"\n",
    "    elif MODEL_ID == 4:\n",
    "        BATCH_SIZE = 64\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-0 #start from learning rate after 40 epochs\n",
    "        ALPHA = 10\n",
    "        model = models.resnet50()\n",
    "        model.fc = nn.Linear(2048, NUM_CLASSES)\n",
    "        modelName = \"resnet50\"\n",
    "    elif MODEL_ID == 5:\n",
    "        BATCH_SIZE = 8\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = models.densenet121()\n",
    "        model.fc = nn.Linear(512, NUM_CLASSES)\n",
    "        modelName = \"densenet121\"\n",
    "    elif MODEL_ID == 6:\n",
    "        BATCH_SIZE = 1024\n",
    "        NUM_EPOCHS = 100\n",
    "        LEARNING_RATE = 1e-1 #start from learning rate after 40 epochs\n",
    "        ALPHA = 6\n",
    "        model = nn.Sequential()\n",
    "        model.add_module(\"linear\", torch.nn.Linear(224*224*3, NUM_CLASSES, bias=False))\n",
    "        # RuntimeError: size mismatch, m1: [172032 x 224], m2: [150528 x 185] at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorMathBlas.cu:249\n",
    "        # model = nn.Linear(224*224*3, NUM_CLASSES) \n",
    "        #error size mismatch, m1: [86016 x 224], m2: [150528 x 185] at /opt/conda/conda-bld/pytorch_1524586445097/work/aten/src/THC/generic/THCTensorMathBlas.cu:249\n",
    "        modelName = \"logisticRegression\"\n",
    "    else:\n",
    "        raise ValueError('Model ID must be an integer between 1 and 6')\n",
    "    return model, modelName, BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d81554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHeadertxt_train(modelName, INPUT_SIZE, filename):\n",
    "    with open(filename, 'a') as a:\n",
    "        a.write('#Epoch\\t\\t   Time\\t\\t            Data\\t\\t   Loss\\t\\t\\t\\t   Prec@1\\t   Prec@5 \\n')\n",
    "\n",
    "def createHeadertxt_dev(modelName, INPUT_SIZE, filename):\n",
    "    with open(filename, 'a') as a:\n",
    "        a.write('#Epoch\\t\\t    Time\\t\\t    Loss\\t\\t   Prec@1\\t\\t   Prec@5 \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02bcd0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # for i, (input, target) in enumerate(train_loader):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        (input,target),(path,_) = data\n",
    "        # measure data loading time\n",
    "        if USE_CUDA:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        if MODEL_ID ==6:\n",
    "            ##flatten input for logistic\n",
    "            input_var = input_var.view(-1,INPUT_SIZE*INPUT_SIZE*3)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5), path=path, minibatch = i)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  '\\Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1, top5=top5)) \n",
    "\n",
    "        if i == (len(train_loader)-1):\n",
    "            with open(filename_train, 'a') as a:\n",
    "                    a.write('{0}\\t'\n",
    "                            '{batch_time.avg:16.3f}\\t'\n",
    "                            '{data_time.avg:16.3f}\\t'\n",
    "                            '{loss.avg:16.4f}\\t'\n",
    "                            '{top1.avg:16.3f}\\t'\n",
    "                            '{top5.avg:16.3f}\\n'.format(\n",
    "                                epoch, batch_time=batch_time,\n",
    "                                data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d93e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation method\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "#    class_correct = list(0. for i in range(NUM_CLASSES))\n",
    "#    class_total = list(0. for i in range(NUM_CLASSES))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if USE_CUDA:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "        with torch.no_grad(): \n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        if MODEL_ID ==6:\n",
    "            ##flatten input for logistic\n",
    "            input_var = input_var.view(-1,INPUT_SIZE*INPUT_SIZE*3)\n",
    "\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1, top5=top5))\n",
    "        if i == (len(val_loader)-1):    \n",
    "            with open(filename_dev, 'a') as a:\n",
    "                    a.write('{0}\\t'\n",
    "                            '{batch_time.avg:16.3f}\\t'\n",
    "                            '{loss.avg:16.4f}\\t'\n",
    "                            '{top1.avg:16.3f}\\t'\n",
    "                            '{top5.avg:16.3f}\\n'.format(\n",
    "                                epoch, batch_time=batch_time,\n",
    "                                loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93234631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print('\\n[INFO] Saved Model to model_best.pth.tar')\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7e8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LEARNING_RATE*0.1** (epoch // ALPHA)\n",
    "    if (lr <= 1e-4): # cap the learning rate to be larger than e-4\n",
    "        lr = 1e-4\n",
    "    print('\\n[Learning Rate] {:0.6f}'.format(lr))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5639b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,), path = None, minibatch = None):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    ## save mislabeled data\n",
    "    if path:\n",
    "        filename = [os.path.basename(p) for p in path]\n",
    "        true_label = [os.path.basename(os.path.dirname(p)) for p in path]\n",
    "        ##debugging for densenet\n",
    "        # print('pred[0] = %s'%pred[0])\n",
    "        ## end debugging\t\t\n",
    "        pred_label = [classes[p] for p in pred[0]]\n",
    "        data = np.array([filename, true_label, pred_label])\n",
    "        out = pd.DataFrame(data.T,columns =['filename', 'true_label','pred_label'])\n",
    "        out.index.name = 'index'\n",
    "        out['correct?'] = out['pred_label']==out['true_label']\n",
    "        out_file = 'predicted_labels.csv'\n",
    "\n",
    "        if os.path.isfile(out_file):\n",
    "            if minibatch==0: # if first minibatch, overwrite existing file\n",
    "                out.to_csv(out_file)\n",
    "            else:\n",
    "                df = pd.read_csv(out_file, index_col = 0)\n",
    "                df = pd.concat([df,out],axis = 0, ignore_index = True)\n",
    "                df.to_csv(out_file)\n",
    "        else: # if file does not exist, make file\n",
    "            out.to_csv(out_file)\n",
    "    \n",
    "    return res\n",
    "\n",
    "class MyImageFolder(datasets.ImageFolder): #return image path and loader\n",
    "    def __getitem__(self, index):\n",
    "        return super(MyImageFolder, self).__getitem__(index), self.imgs[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bad7e195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--modelid'], dest='modelid', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help='1(resnet18), 2(VGG16), 3(resnet101), 4(densenet121)', metavar='MODEL_ID')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch LeafSnap Training')\n",
    "parser.add_argument('--resume', required = True, type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (type '' for none)')\n",
    "parser.add_argument('--modelid', required = True, type=int, metavar='MODEL_ID',\n",
    "                    help='1(resnet18), 2(VGG16), 3(resnet101), 4(densenet121)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f76bd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: [-h] --resume PATH --modelid MODEL_ID\n",
      ": error: the following arguments are required: --resume, --modelid\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()\n",
    "MODEL_ID = args.modelid \n",
    "print('\\n[INFO] Creating Model')\n",
    "model, modelName, BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, ALPHA = selectModel(MODEL_ID)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if USE_CUDA:\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    criterion = criterion.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE,\n",
    "                     momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), \n",
    "#                       eps=1e-08, weight_decay=1e-4)\n",
    "\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "        print(\"=> Loaded model Prec1 = %0.2f%%\"%best_prec1)\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "print('\\n[INFO] Reading Training and Testing Dataset')\n",
    "traindir = os.path.join('dataset', 'train_%d_augment'%INPUT_SIZE)\n",
    "testdir = os.path.join('dataset', 'test_%d'%INPUT_SIZE)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "data_train = MyImageFolder(traindir, transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]))\n",
    "data_test = datasets.ImageFolder(testdir, transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize]))\n",
    "classes = data_train.classes\n",
    "classes_test = data_test.classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(data_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=4) \n",
    "\n",
    "print('\\n[INFO] Preparing txt files to save epoch data')\n",
    "timestamp_string = time.strftime(\"%Y%m%d-%H%M%S\") \n",
    "filename_train = './dataAndPlots/' + timestamp_string + '_train' + '_' + modelName + '_' + str(INPUT_SIZE) + '.txt'\n",
    "filename_dev = './dataAndPlots/' + timestamp_string + '_dev' + '_' + modelName + '_' + str(INPUT_SIZE) + '.txt'\n",
    "createHeadertxt_train(modelName, INPUT_SIZE, filename_train)\n",
    "createHeadertxt_dev(modelName, INPUT_SIZE, filename_dev)\n",
    "\n",
    "print('\\n[INFO] Training Started')\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    print('\\n[INFO] Saved Model to leafsnap_model.pth')    \n",
    "    torch.save(model, 'leafsnap_model.pth')\n",
    "\n",
    "print('\\n[DONE]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135cdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
